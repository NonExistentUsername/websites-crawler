{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpeRqEswUaZQ"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import uuid\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bWd8YWixVne9",
    "outputId": "3ca8ed5e-a911-4941-c44e-a3aaa0442fa0"
   },
   "outputs": [],
   "source": [
    "!pip install redis\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-SKIo_sVh_x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import redis.asyncio as redis\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from redis.commands.search.field import TextField, VectorField\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "from redis.commands.search.query import Query\n",
    "from torch import Tensor\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqOdInSgVmSO"
   },
   "outputs": [],
   "source": [
    "REDIS_DB = 0\n",
    "REDIS_HOST = \"srv-captain--search-db\"\n",
    "REDIS_PORT = 6379\n",
    "REDIS_PASSWORD = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOcTAr-6VzL2"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"andersonbcdefg/bge-small-4096\"\n",
    "VECTOR_DIMENSION = 384\n",
    "TOKENS_LIMIT = 4096 - 16  # To be safe\n",
    "DEVICE = \"cuda\"\n",
    "INDEX_NAME = \"idx:pages_vss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7XXTZSOV7c_",
    "outputId": "1f486391-9603-4d46-fbbf-6c5ba4fc036d"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME, truncation=True)\n",
    "model = AutoModel.from_pretrained(EMBEDDING_MODEL_NAME).half().to(DEVICE)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"feature-extraction\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVsDMf6VV8oj"
   },
   "outputs": [],
   "source": [
    "def get_redis_client():\n",
    "    return redis.Redis(\n",
    "        host=REDIS_HOST,\n",
    "        port=REDIS_PORT,\n",
    "        db=REDIS_DB,\n",
    "        password=REDIS_PASSWORD,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7ATjk_KWBc-"
   },
   "outputs": [],
   "source": [
    "def merge_embeddings(embeddings):\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    # Merge embeddings\n",
    "    embeddings = embeddings.mean(dim=0)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def average_pool(states: Tensor) -> Tensor:\n",
    "    return states.mean(dim=0)\n",
    "\n",
    "\n",
    "def prepare_text(text: str) -> list[str]:\n",
    "    tokens = tokenizer(text, padding=False, truncation=False)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens[\"input_ids\"]), TOKENS_LIMIT):\n",
    "        chunk = {\n",
    "            \"input_ids\": tokens[\"input_ids\"][i : i + TOKENS_LIMIT],\n",
    "            \"attention_mask\": tokens[\"attention_mask\"][i : i + TOKENS_LIMIT],\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    texts = []\n",
    "    for chunk in chunks:\n",
    "        text = tokenizer.decode(\n",
    "            chunk[\"input_ids\"],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )\n",
    "        texts.append(text)\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n",
    "async def put_crawled_url(url: str, embedings):\n",
    "    client = get_redis_client()\n",
    "    result = await client.hset(\n",
    "        f\"pages:{url}\", mapping={\"url\": url, \"embeddings\": embedings}\n",
    "    )\n",
    "    await client.aclose()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_go0J1sKWFTJ"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(text: str) -> Tensor:\n",
    "    texts = prepare_text(text)\n",
    "\n",
    "    outputs: List[List[float]] = []\n",
    "\n",
    "    for text in texts:\n",
    "        output = pipe(text)[0]\n",
    "        outputs.extend(output)\n",
    "\n",
    "    embeddings_list = torch.tensor(outputs)\n",
    "\n",
    "    return average_pool(embeddings_list).cpu().numpy().astype(np.float32).tobytes()\n",
    "\n",
    "\n",
    "async def put_url(url: str, text: str):\n",
    "    embedings = get_embeddings(text)\n",
    "    return await put_crawled_url(url, embedings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qH-9N7uWJ4L"
   },
   "outputs": [],
   "source": [
    "DB_HOST = \"srv-captain--crawler-db\"\n",
    "DB_PORT = 5432\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"b98ca22959171351\"\n",
    "DB_NAME = \"websites\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "soqe3q3MW0GQ",
    "outputId": "69e3147c-56ce-4c9e-c6ab-9361dc94f54d"
   },
   "outputs": [],
   "source": [
    "!pip install aiopg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwjrEQElXRjR"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import aiopg\n",
    "\n",
    "# import psycopg2  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNejAXfkWgnw"
   },
   "outputs": [],
   "source": [
    "class AsyncDB:\n",
    "    def __init__(self, host, port, user, password, database):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.database = database\n",
    "\n",
    "        self.dsn = (\n",
    "            f\"dbname={database} user={user} password={password} host={host} port={port}\"\n",
    "        )\n",
    "\n",
    "    async def insert(self, table, columns, values):\n",
    "        async with aiopg.create_pool(self.dsn) as pool:\n",
    "            async with pool.acquire() as conn:\n",
    "                async with conn.cursor() as cursor:\n",
    "                    await cursor.execute(\n",
    "                        f\"INSERT INTO {table} ({', '.join(columns)}) VALUES ({', '.join(['%s'] * len(values))})\",\n",
    "                        values,\n",
    "                    )\n",
    "\n",
    "    async def select(\n",
    "        self, table, columns, where=None, limit=None, offset=None, order_by=None\n",
    "    ):\n",
    "        query = f\"SELECT {', '.join(columns)} FROM {table}\"\n",
    "        if where:\n",
    "            query += f\" WHERE {where}\"\n",
    "        if order_by:\n",
    "            query += f\" ORDER BY {order_by}\"\n",
    "        if limit:\n",
    "            query += f\" LIMIT {limit}\"\n",
    "        if offset:\n",
    "            query += f\" OFFSET {offset}\"\n",
    "\n",
    "        async with aiopg.create_pool(self.dsn) as pool:\n",
    "            async with pool.acquire() as conn:\n",
    "                async with conn.cursor() as cursor:\n",
    "                    async with cursor.begin():\n",
    "                        await cursor.execute(query)\n",
    "                        return await cursor.fetchall()\n",
    "\n",
    "    async def count(self, table):\n",
    "        async with aiopg.create_pool(self.dsn) as pool:\n",
    "            async with pool.acquire() as conn:\n",
    "                async with conn.cursor() as cursor:\n",
    "                    async with cursor.begin():\n",
    "                        await cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "                        return await cursor.fetchone()\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def create_db() -> AsyncDB:\n",
    "    return AsyncDB(DB_HOST, DB_PORT, DB_USER, DB_PASSWORD, DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AUL6agAXhgu1"
   },
   "outputs": [],
   "source": [
    "def get_embeddings_batch(batch: List[str], batch_size=512) -> List[Tensor]:\n",
    "    texts = [prepare_text(text) for text in batch]\n",
    "    sizes = [len(text) for text in texts]\n",
    "\n",
    "    sizes_extended = [0] * len(texts)\n",
    "    texts = [t for text in texts for t in text]\n",
    "    indexes = []\n",
    "    \n",
    "    for cur_index, size in enumerate(sizes):\n",
    "        indexes.extend([cur_index] * size)\n",
    "\n",
    "    # print(indexes)\n",
    "\n",
    "    embeddings: List[Tensor] = []\n",
    "\n",
    "    for index in range(0, len(texts), batch_size):\n",
    "        texts_batch = texts[index : index + batch_size]\n",
    "        output = pipe(texts_batch)\n",
    "        for subindex, item in enumerate(output):\n",
    "            embeddings.extend(item[0])\n",
    "            cur_index = indexes[index + subindex]\n",
    "            added = len(item[0])\n",
    "            sizes_extended[cur_index] += added\n",
    "\n",
    "    # print(sizes_extended)\n",
    "\n",
    "    merged_embeddings = []\n",
    "\n",
    "    # def rec_print(depth, pos, value):        \n",
    "    #     if pos < len(value) and isinstance(value[pos], list):\n",
    "    #         if len(value[pos]) != 384:\n",
    "    #             print('|'+('-'*depth)+f'depth:{depth} - {len(value[pos])}')\n",
    "    #             rec_print(depth + 1, 0, value[pos])\n",
    "    #             rec_print(depth, pos + 1, value)\n",
    "\n",
    "    shift = 0\n",
    "    for size in sizes_extended:\n",
    "        embeddings_chunk = embeddings[shift : shift + size]\n",
    "        embeddings_chunk = [torch.tensor(embeddings) for embeddings in embeddings_chunk]\n",
    "\n",
    "        tensor = torch.vstack(embeddings_chunk).to(DEVICE)\n",
    "        # print(tensor.shape)\n",
    "        tensor = average_pool(tensor)\n",
    "        # print(tensor.shape)\n",
    "        # print(tensor.cpu().numpy().shape)\n",
    "\n",
    "        embeddings_chunk = (\n",
    "            tensor\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "            .astype(np.float32)\n",
    "            .tobytes()\n",
    "        )\n",
    "\n",
    "        merged_embeddings.append(embeddings_chunk)\n",
    "        shift += size\n",
    "\n",
    "    return merged_embeddings\n",
    "\n",
    "\n",
    "async def put_crawled_url(client, url: str, embedings):\n",
    "    result = await client.hset(\n",
    "        f\"pages:{url}\", mapping={\"url\": url, \"embeddings\": embedings}\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "async def get_crawled_url(url: str):\n",
    "    client = get_redis_client()\n",
    "    result = await client.ft(INDEX_NAME).load_document(f\"pages:{url}\")\n",
    "    await client.aclose()\n",
    "    return result\n",
    "\n",
    "\n",
    "async def get_all_urls_from_redis():\n",
    "    client = get_redis_client()\n",
    "    keys = await client.keys(\"pages:*\")\n",
    "    urls = [key.decode(\"utf-8\")[6:] for key in keys]\n",
    "    await client.aclose()\n",
    "    return set(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def move_to_redis(start_offset: int = 0):\n",
    "    start = time.time()\n",
    "    db = create_db()\n",
    "    client = get_redis_client()\n",
    "\n",
    "    total_count = await db.count(\"websites\")\n",
    "    total_count = total_count[0]\n",
    "    print(f\"Total count: {total_count}\")\n",
    "    put_chunk_size = 128\n",
    "    page_size = put_chunk_size * 2\n",
    "\n",
    "    already_moved = await get_all_urls_from_redis()\n",
    "    print(f\"Initialized in {time.time() - start} seconds\")\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(start_offset, total_count, page_size):\n",
    "        start = time.time()\n",
    "\n",
    "        urls = await db.select(\n",
    "            \"websites\", [\"url\", \"content\"], order_by=\"url\", limit=page_size, offset=i\n",
    "        )\n",
    "        print(f\"Selected {len(urls)} urls in {time.time() - start} seconds\")\n",
    "        start = time.time()\n",
    "        cur_len = len(urls)\n",
    "        urls = [\n",
    "            (url, content)\n",
    "            for url, content in urls\n",
    "            if url not in already_moved\n",
    "            and content is not None\n",
    "            and str(content).strip() != \"\"\n",
    "        ]\n",
    "        print(f\"Filtered {cur_len - len(urls)} urls in {time.time() - start} seconds\")\n",
    "        start = time.time()\n",
    "\n",
    "        for chunk in range(0, len(urls), put_chunk_size):\n",
    "            batch = list(urls[chunk : chunk + put_chunk_size])\n",
    "            # remove empty texts\n",
    "            batch = [(url, text) for url, text in batch if text]\n",
    "            texts = [text for _, text in batch]\n",
    "            temp_urls = [url for url, _ in batch]\n",
    "\n",
    "            embeddings = get_embeddings_batch(texts)\n",
    "            torch.cuda.empty_cache()\n",
    "            tasks = [\n",
    "                asyncio.create_task(\n",
    "                    put_crawled_url(client, temp_urls[i], embeddings[i])\n",
    "                )\n",
    "                for i in range(len(texts))\n",
    "            ]\n",
    "            await asyncio.wait(tasks, return_when=asyncio.ALL_COMPLETED)\n",
    "\n",
    "        print(f\"Moved {len(urls)} urls in {time.time() - start} seconds\")\n",
    "        print(f\"Total moved: {i + page_size}\")\n",
    "        start = time.time()\n",
    "\n",
    "    await client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 106126\n",
      "Initialized in 0.05830836296081543 seconds\n",
      "Selected 256 urls in 0.04970359802246094 seconds\n",
      "Filtered 0 urls in 0.0009067058563232422 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 256 urls in 61.469452142715454 seconds\n",
      "Total moved: 256\n",
      "Selected 256 urls in 0.0402836799621582 seconds\n",
      "Filtered 0 urls in 0.0007066726684570312 seconds\n",
      "Moved 256 urls in 59.27594757080078 seconds\n",
      "Total moved: 512\n",
      "Selected 256 urls in 0.04438304901123047 seconds\n",
      "Filtered 128 urls in 0.000858306884765625 seconds\n",
      "Moved 128 urls in 34.091941118240356 seconds\n",
      "Total moved: 768\n",
      "Selected 256 urls in 0.15420293807983398 seconds\n",
      "Filtered 0 urls in 0.0013079643249511719 seconds\n",
      "Moved 256 urls in 63.21256685256958 seconds\n",
      "Total moved: 1024\n",
      "Selected 256 urls in 0.1997528076171875 seconds\n",
      "Filtered 0 urls in 0.0028731822967529297 seconds\n",
      "Moved 256 urls in 133.3040235042572 seconds\n",
      "Total moved: 1280\n",
      "Selected 256 urls in 0.36730098724365234 seconds\n",
      "Filtered 0 urls in 0.008798837661743164 seconds\n"
     ]
    }
   ],
   "source": [
    "await move_to_redis(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvA8e4S-XJxA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
