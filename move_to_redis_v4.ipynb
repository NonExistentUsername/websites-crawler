{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xpeRqEswUaZQ"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import uuid\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bWd8YWixVne9",
    "outputId": "3ca8ed5e-a911-4941-c44e-a3aaa0442fa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: redis in /opt/conda/lib/python3.10/site-packages (5.0.4)\n",
      "Requirement already satisfied: async-timeout>=4.0.3 in /opt/conda/lib/python3.10/site-packages (from redis) (4.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.40.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install redis\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q-SKIo_sVh_x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import redis.asyncio as redis\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from redis.commands.search.field import TextField, VectorField\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "from redis.commands.search.query import Query\n",
    "from torch import Tensor\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cqOdInSgVmSO"
   },
   "outputs": [],
   "source": [
    "REDIS_DB = 0\n",
    "REDIS_HOST = \"94537961.xyz\"\n",
    "REDIS_PORT = 6379\n",
    "REDIS_PASSWORD = \"0f8G0s9aokzjBh5B6W9ZGLUo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wOcTAr-6VzL2"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"andersonbcdefg/bge-small-4096\"\n",
    "VECTOR_DIMENSION = 384\n",
    "TOKENS_LIMIT = 4096 - 16  # To be safe\n",
    "DEVICE = \"cuda\"\n",
    "INDEX_NAME = \"idx:pages_vss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7XXTZSOV7c_",
    "outputId": "1f486391-9603-4d46-fbbf-6c5ba4fc036d"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME, truncation=True)\n",
    "model = AutoModel.from_pretrained(EMBEDDING_MODEL_NAME).half().to(DEVICE)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"feature-extraction\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SVsDMf6VV8oj"
   },
   "outputs": [],
   "source": [
    "def get_redis_client():\n",
    "    return redis.Redis(\n",
    "        host=REDIS_HOST,\n",
    "        port=REDIS_PORT,\n",
    "        db=REDIS_DB,\n",
    "        password=REDIS_PASSWORD,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "J7ATjk_KWBc-"
   },
   "outputs": [],
   "source": [
    "def merge_embeddings(embeddings):\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    # Merge embeddings\n",
    "    embeddings = embeddings.mean(dim=0)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def average_pool(states: Tensor) -> Tensor:\n",
    "    return states.mean(dim=0)\n",
    "\n",
    "\n",
    "def prepare_text(text: str):\n",
    "    tokens = tokenizer(text, padding=False, truncation=False)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens[\"input_ids\"]), TOKENS_LIMIT):\n",
    "        chunk = {\n",
    "            \"input_ids\": tokens[\"input_ids\"][i : i + TOKENS_LIMIT],\n",
    "            \"attention_mask\": tokens[\"attention_mask\"][i : i + TOKENS_LIMIT],\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    texts = []\n",
    "    for chunk in chunks:\n",
    "        text = tokenizer.decode(\n",
    "            chunk[\"input_ids\"],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )\n",
    "        texts.append(text)\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n",
    "async def put_crawled_url(url: str, embedings):\n",
    "    client = get_redis_client()\n",
    "    result = await client.hset(\n",
    "        f\"pages:{url}\", mapping={\"url\": url, \"embeddings\": embedings}\n",
    "    )\n",
    "    await client.aclose()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_go0J1sKWFTJ"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(text: str) -> Tensor:\n",
    "    texts = prepare_text(text)\n",
    "\n",
    "    outputs: List[List[float]] = []\n",
    "\n",
    "    for text in texts:\n",
    "        output = pipe(text)[0]\n",
    "        outputs.extend(output)\n",
    "\n",
    "    embeddings_list = torch.tensor(outputs)\n",
    "\n",
    "    return average_pool(embeddings_list).cpu().numpy().astype(np.float32).tobytes()\n",
    "\n",
    "\n",
    "async def put_url(url: str, text: str):\n",
    "    embedings = get_embeddings(text)\n",
    "    return await put_crawled_url(url, embedings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2qH-9N7uWJ4L"
   },
   "outputs": [],
   "source": [
    "DB_HOST = \"94537961.xyz\"\n",
    "DB_PORT = 6543\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"bcf8cd1cde6347fe\"\n",
    "DB_NAME = \"sites\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "soqe3q3MW0GQ",
    "outputId": "69e3147c-56ce-4c9e-c6ab-9361dc94f54d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiopg in /opt/conda/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: psycopg2-binary>=2.9.5 in /opt/conda/lib/python3.10/site-packages (from aiopg) (2.9.9)\n",
      "Requirement already satisfied: async-timeout<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from aiopg) (4.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install aiopg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IwjrEQElXRjR"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import aiopg\n",
    "\n",
    "# import psycopg2  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iNejAXfkWgnw"
   },
   "outputs": [],
   "source": [
    "class AsyncDB:\n",
    "    def __init__(self, host, port, user, password, database):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.database = database\n",
    "\n",
    "        self.dsn = (\n",
    "            f\"dbname={database} user={user} password={password} host={host} port={port}\"\n",
    "        )\n",
    "\n",
    "    async def insert(self, table, columns, values):\n",
    "        async with aiopg.create_pool(self.dsn) as pool:\n",
    "            async with pool.acquire() as conn:\n",
    "                async with conn.cursor() as cursor:\n",
    "                    await cursor.execute(\n",
    "                        f\"INSERT INTO {table} ({', '.join(columns)}) VALUES ({', '.join(['%s'] * len(values))})\",\n",
    "                        values,\n",
    "                    )\n",
    "\n",
    "    async def select(\n",
    "        self, table, columns, where=None, limit=None, offset=None, order_by=None\n",
    "    ):\n",
    "        query = f\"SELECT {', '.join(columns)} FROM {table}\"\n",
    "        if where:\n",
    "            query += f\" WHERE {where}\"\n",
    "        if order_by:\n",
    "            query += f\" ORDER BY {order_by}\"\n",
    "        if limit:\n",
    "            query += f\" LIMIT {limit}\"\n",
    "        if offset:\n",
    "            query += f\" OFFSET {offset}\"\n",
    "\n",
    "        async with aiopg.create_pool(self.dsn) as pool:\n",
    "            async with pool.acquire() as conn:\n",
    "                async with conn.cursor() as cursor:\n",
    "                    async with cursor.begin():\n",
    "                        await cursor.execute(query)\n",
    "                        return await cursor.fetchall()\n",
    "\n",
    "    async def count(self, table):\n",
    "        async with aiopg.create_pool(self.dsn) as pool:\n",
    "            async with pool.acquire() as conn:\n",
    "                async with conn.cursor() as cursor:\n",
    "                    async with cursor.begin():\n",
    "                        await cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "                        return await cursor.fetchone()\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def create_db() -> AsyncDB:\n",
    "    return AsyncDB(DB_HOST, DB_PORT, DB_USER, DB_PASSWORD, DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AUL6agAXhgu1"
   },
   "outputs": [],
   "source": [
    "def get_embeddings_batch(batch: List[str]) -> List[Tensor]:\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    texts = [prepare_text(text) for text in batch]\n",
    "    sizes = [len(text) for text in texts]\n",
    "    texts = [text for text in texts for text in text]\n",
    "\n",
    "    embeddings: List[Tensor] = []\n",
    "\n",
    "    for index in range(0, len(texts), batch_size):\n",
    "        texts_batch = texts[index : index + batch_size]\n",
    "        output = pipe(texts_batch)\n",
    "\n",
    "        iterator = iter(output)\n",
    "\n",
    "        for chunk_size in sizes:\n",
    "            chunk = []\n",
    "            for _ in range(chunk_size):\n",
    "                chunk.extend(next(iterator)[0])\n",
    "\n",
    "            embeddings.append(\n",
    "                average_pool(torch.tensor(chunk).to(DEVICE))\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "                .astype(np.float32)\n",
    "                .tobytes()\n",
    "            )\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "async def put_crawled_url(client, url: str, embedings):\n",
    "    result = await client.hset(\n",
    "        f\"pages:{url}\", mapping={\"url\": url, \"embeddings\": embedings}\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "async def get_crawled_url(url: str):\n",
    "    client = get_redis_client()\n",
    "    result = await client.ft(INDEX_NAME).load_document(f\"pages:{url}\")\n",
    "    await client.aclose()\n",
    "    return result\n",
    "\n",
    "\n",
    "async def get_all_urls_from_redis():\n",
    "    client = get_redis_client()\n",
    "    keys = await client.keys(\"pages:*\")\n",
    "    urls = [key.decode(\"utf-8\")[6:] for key in keys]\n",
    "    await client.aclose()\n",
    "    return set(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DC7TM3ZOWzcJ"
   },
   "outputs": [],
   "source": [
    "async def move_to_redis(start_offset: int = 0):\n",
    "    start = time.time()\n",
    "    db = create_db()\n",
    "    client = get_redis_client()\n",
    "\n",
    "    total_count = await db.count(\"websites\")\n",
    "    total_count = total_count[0]\n",
    "    print(f\"Total count: {total_count}\")\n",
    "    put_chunk_size = 128\n",
    "    page_size = put_chunk_size * 2\n",
    "\n",
    "    already_moved = await get_all_urls_from_redis()\n",
    "    print(f\"Initialized in {time.time() - start} seconds\")\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(start_offset, total_count, page_size):\n",
    "        start = time.time()\n",
    "\n",
    "        urls = await db.select(\n",
    "            \"websites\", [\"url\", \"content\"], order_by=\"url\", limit=page_size, offset=i\n",
    "        )\n",
    "        print(f\"Selected {len(urls)} urls in {time.time() - start} seconds\")\n",
    "        start = time.time()\n",
    "        cur_len = len(urls)\n",
    "        urls = [\n",
    "            (url, content)\n",
    "            for url, content in urls\n",
    "            if url not in already_moved\n",
    "            and content is not None\n",
    "            and str(content).strip() != \"\"\n",
    "        ]\n",
    "        print(f\"Filtered {cur_len - len(urls)} urls in {time.time() - start} seconds\")\n",
    "        start = time.time()\n",
    "\n",
    "        for chunk in range(0, len(urls), put_chunk_size):\n",
    "            batch = list(urls[chunk : chunk + put_chunk_size])\n",
    "            texts = [text for _, text in batch]\n",
    "            temp_urls = [url for url, _ in batch]\n",
    "\n",
    "            embeddings = get_embeddings_batch(texts)\n",
    "            torch.cuda.empty_cache()\n",
    "            tasks = [\n",
    "                asyncio.create_task(\n",
    "                    put_crawled_url(client, temp_urls[i], embeddings[i])\n",
    "                )\n",
    "                for i in range(len(texts))\n",
    "            ]\n",
    "            await asyncio.wait(tasks, return_when=asyncio.ALL_COMPLETED)\n",
    "\n",
    "        print(f\"Moved {len(urls)} urls in {time.time() - start} seconds\")\n",
    "        print(f\"Total moved: {i + page_size}\")\n",
    "        start = time.time()\n",
    "\n",
    "    await client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMuEOYfJXFfx",
    "outputId": "41b7f20b-83a5-4c04-d70d-55c076117a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 2302237\n",
      "Initialized in 35.946128129959106 seconds\n"
     ]
    }
   ],
   "source": [
    "await move_to_redis(2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvA8e4S-XJxA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
